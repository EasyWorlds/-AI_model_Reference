{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: google drive mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSF7rY1srQt0",
        "outputId": "7abb0371-02e0-4004-ce35-dd73524bd9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cr0t4YjirRks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBqJaS0NquVk",
        "outputId": "5bee5aba-8c62-421f-88e7-3c76ed2a30ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/17, Loss: 0.8437826603651046\n",
            "Epoch 2/17, Loss: 0.26745012588799\n",
            "Epoch 3/17, Loss: 0.20186726885537307\n",
            "Epoch 4/17, Loss: 0.09262221672882637\n",
            "Epoch 5/17, Loss: 0.09571815395417313\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 6/17, Loss: 0.08501489725507175\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 7/17, Loss: 0.033041211970460914\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 8/17, Loss: 0.03452343909302726\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 9/17, Loss: 0.0924679522557805\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 10/17, Loss: 0.01700935799550886\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 11/17, Loss: 0.026282797466653088\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 12/17, Loss: 0.046500966225479105\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 13/17, Loss: 0.02938907714560628\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 14/17, Loss: 0.022159177217205676\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 15/17, Loss: 0.02068830677793206\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 16/17, Loss: 0.022364748428420473\n",
            "Accuracy on test set: 100.0%\n",
            "Epoch 17/17, Loss: 0.014718638104386627\n",
            "Accuracy on test set: 100.0%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# 데이터 경로 설정\n",
        "train_data_path = \"/content/drive/MyDrive/AIR/aug\"\n",
        "test_data_path = \"/content/drive/MyDrive/AIR/test\"\n",
        "\n",
        "# 선택한 3개의 클래스\n",
        "selected_classes = ['0', '1', '2']\n",
        "\n",
        "# 데이터 전처리 및 증강\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_dataset = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
        "\n",
        "# Filter out data for only the selected classes\n",
        "train_dataset.samples = [\n",
        "    (image, label) for image, label in train_dataset.samples if train_dataset.classes[label] in selected_classes\n",
        "]\n",
        "train_dataset.targets = [label for _, label in train_dataset.samples]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.ImageFolder(root=test_data_path, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# ResNet 모델 불러오기\n",
        "model = models.resnet18(pretrained=True)\n",
        "# 마지막 Fully Connected Layer를 선택한 3개의 클래스에 맞게 조정\n",
        "model.fc = nn.Linear(model.fc.in_features, len(selected_classes))\n",
        "\n",
        "# 손실 함수 및 최적화 함수 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 학습\n",
        "num_epochs = 17\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    if epoch>=5:\n",
        "       torch.save(model.state_dict(), f\"/content/drive/MyDrive/AIR/resnet_pre_epoch_{epoch}.pth\")\n",
        "       # 테스트\n",
        "       model.eval()\n",
        "       correct = 0\n",
        "       total = 0\n",
        "\n",
        "       with torch.no_grad():\n",
        "             for inputs, labels in test_loader:\n",
        "                 inputs, labels = inputs.to(device), labels.to(device)\n",
        "                 outputs = model(inputs)\n",
        "                 _, predicted = torch.max(outputs.data, 1)\n",
        "                 total += labels.size(0)\n",
        "                 correct += (predicted == labels).sum().item()\n",
        "\n",
        "             print(f\"Accuracy on test set: {100 * correct / total}%\")\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "#torch.save(model.state_dict(), \"/content/drive/MyDrive/AIR/resnet_model_selected_classes.pth\")\n",
        "\n",
        "# 테스트\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test set: {100 * correct / total}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 모델 인스턴스 생성\n",
        "model = CustomResNet18(num_classes=3)\n",
        "\n",
        "# 학습된 가중치 로드\n",
        "state_dict = torch.load(\"/content/drive/MyDrive/AIR/resnet__nonpre_epoch_17.pth\")\n",
        "# 모델의 state_dict와 불러온 state_dict의 키를 비교해 일치하도록 수정\n",
        "model_dict = model.state_dict()\n",
        "state_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
        "model_dict.update(state_dict)\n",
        "model.load_state_dict(model_dict)\n",
        "# 테스트\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test set: {100 * correct / total}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "CiiEBBWyO8Jv",
        "outputId": "35336dce-8857-452f-da1a-e739f483c54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/AIR/resnet__nonpre_epoch_17.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-2ac5b81a32a1>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 학습된 가중치 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/AIR/resnet__nonpre_epoch_17.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# 모델의 state_dict와 불러온 state_dict의 키를 비교해 일치하도록 수정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/AIR/resnet__nonpre_epoch_17.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# 모델 정의\n",
        "class CustomResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(CustomResNet18, self).__init__()\n",
        "        self.resnet18 = models.resnet18(pretrained=False)\n",
        "        in_features = self.resnet18.fc.in_features\n",
        "        self.resnet18.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet18(x)\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model = CustomResNet18(num_classes=3)\n",
        "\n",
        "# 학습된 가중치 로드\n",
        "state_dict = torch.load(\"/content/drive/MyDrive/AIR/resnet_model_selected_classes.pth\")\n",
        "# 모델의 state_dict와 불러온 state_dict의 키를 비교해 일치하도록 수정\n",
        "model_dict = model.state_dict()\n",
        "state_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
        "model_dict.update(state_dict)\n",
        "model.load_state_dict(model_dict)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# 이미지 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# 분류할 이미지 경로\n",
        "image_path = \"/content/drive/MyDrive/AIR/test/1/wing1.jpg\"\n",
        "\n",
        "# 이미지 불러오기 및 전처리\n",
        "image = Image.open(image_path)\n",
        "input_tensor = transform(image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "start_time = time.time()\n",
        "# 모델에 입력 전달 및 예측\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "\n",
        "# 예측 결과 확인\n",
        "_, predicted_class = torch.max(output, 1)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(\"Predicted class:\", predicted_class.item())\n",
        "print(f\"Execution Time: {execution_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpcnIvHJ3xrI",
        "outputId": "66c13712-45e1-4fcd-dba0-d25f101a8b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 0\n",
            "Execution Time: 0.09061145782470703 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPOD6Gwlc6bt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}